{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NandGate.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/yepuv1/tensorflow/blob/master/NandGate.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "h35Fgz8M9Bie",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import math\n",
        "import numpy as np\n",
        "import h5py\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.python.framework import ops\n",
        "\n",
        "%matplotlib inline\n",
        "np.random.seed(1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jhBuGIkuJvXw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def nand(a, b):\n",
        "  return np.logical_not(np.logical_and(a,b))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rXRLxSISMLN-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def generate_nand_data(examples_size, seed=1):\n",
        "  \n",
        "  assert(examples_size >= 4)\n",
        "  nand_input = np.array([[0,0],[0,1],[1,0],[1,1]], np.bool)\n",
        "  \n",
        "  features_size = 2\n",
        "  examples_size = examples_size - examples_size % 4\n",
        "  partition_size = np.int32(examples_size / 4)\n",
        "\n",
        "  X = np.zeros(shape=(features_size,examples_size), dtype=np.int32)\n",
        "  Y = np.zeros(shape=(1,examples_size), dtype=np.int32)\n",
        "  \n",
        "  for i in range(0,4):\n",
        "    s, e = (partition_size*i, partition_size*(i+1) )\n",
        "    X[:,s:e] = X[:,s:e] + nand_input[i].reshape(1,2).T\n",
        "  \n",
        "  Y = nand(X[0,:], X[1,:]).astype(np.int32).reshape(1,examples_size)\n",
        "  \n",
        "  return X, Y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-VpxQ1zMgtPt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def one_hot_matrix(labels, C):\n",
        "    \"\"\"\n",
        "    Creates a matrix where the i-th row corresponds to the ith class number and the jth column\n",
        "                     corresponds to the jth training example. So if example j had a label i. Then entry (i,j) \n",
        "                     will be 1. \n",
        "                     \n",
        "    Arguments:\n",
        "    labels -- vector containing the labels \n",
        "    C -- number of classes, the depth of the one hot dimension\n",
        "    \n",
        "    Returns: \n",
        "    one_hot -- one hot matrix\n",
        "    \"\"\"\n",
        "    \n",
        "    C = tf.constant(dtype=tf.int32, name=\"C\", value=C)\n",
        "    one_hot_matrix = tf.one_hot(labels,depth=C, axis=0)\n",
        "    with tf.Session() as sess:\n",
        "      one_hot = sess.run(one_hot_matrix)\n",
        "   \n",
        "    return one_hot"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ynl7o7519uSR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def create_placeholders(n_x, n_y):\n",
        "    \"\"\"\n",
        "    Creates the placeholders for the tensorflow session.\n",
        "    \n",
        "    Arguments:\n",
        "    n_x -- scalar, size of input vector \n",
        "    n_y -- scalar, number of classes (from 0 to 1, so -> 2)\n",
        "    \n",
        "    Returns:\n",
        "    X -- placeholder for the data input, of shape [n_x, None] and dtype \"float\"\n",
        "    Y -- placeholder for the input labels, of shape [n_y, None] and dtype \"float\"\n",
        "    \n",
        "    Tips:\n",
        "    - You will use None because it let's us be flexible on the number of examples you will for the placeholders.\n",
        "      In fact, the number of examples during test/train is different.\n",
        "    \"\"\"\n",
        "\n",
        "    X = tf.placeholder(tf.float32, shape=(n_x, None))\n",
        "    Y = tf.placeholder(tf.float32, shape=(n_y, None))\n",
        "\n",
        "    \n",
        "    return X, Y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "auX2nNLs_F7D",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def initialize_parameters(dic_of_layer_sizes):\n",
        "    \"\"\"\n",
        "    Initializes parameters to build a neural network with tensorflow. The shapes are:\n",
        "                        W1 : [25, 2]\n",
        "                        b1 : [25, 1]\n",
        "                        W2 : [12, 25]\n",
        "                        b2 : [12, 1]\n",
        "                        W3 : [6, 12]\n",
        "                        b3 : [6, 1]\n",
        "    \n",
        "    Returns:\n",
        "    parameters -- a dictionary of tensors containing W1, b1, W2, b2, W3, b3\n",
        "    \"\"\"\n",
        "    \n",
        "    tf.set_random_seed(1) \n",
        "        \n",
        "    \n",
        "    parameters ={}\n",
        "    for key, value in dic_of_layer_sizes.items():\n",
        "      print(key)\n",
        "      print(value)\n",
        "      parameters[key]=tf.get_variable(key, value, initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
        "    \n",
        "    \n",
        "    return parameters"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6ucbcR9OK_ur",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def forward_propagation(X, parameters):\n",
        "    \"\"\"\n",
        "    Implements the forward propagation for the model: LINEAR -> RELU -> LINEAR ->  SOFTMAX\n",
        "    \n",
        "    Arguments:\n",
        "    X -- input dataset placeholder, of shape (input size, number of examples)\n",
        "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", \"W2\", \"b2\", \"W3\", \"b3\"\n",
        "                  the shapes are given in initialize_parameters\n",
        "\n",
        "    Returns:\n",
        "    Z3 -- the output of the last LINEAR unit\n",
        "    \"\"\"\n",
        "    \n",
        "    # Retrieve the parameters from the dictionary \"parameters\" \n",
        "    W1 = parameters['W1']\n",
        "    b1 = parameters['b1']\n",
        "    W2 = parameters['W2']\n",
        "    b2 = parameters['b2']\n",
        "              \n",
        "    Z1 = tf.add(tf.matmul(W1, X), b1)  # LINEAR                                         \n",
        "    A1 = tf.nn.relu(Z1)                # RELU                           \n",
        "    Z2 = tf.add(tf.matmul(W2, A1), b2) # LINEAR                                           \n",
        "\n",
        "    return Z2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "O2HSASNFMu45",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def compute_cost(Z3, Y):\n",
        "    \"\"\"\n",
        "    Computes the cost\n",
        "    \n",
        "    Arguments:\n",
        "    Z3 -- output of forward propagation (output of the last LINEAR unit), of shape (6, number of examples)\n",
        "    Y -- \"true\" labels vector placeholder, same shape as Z3\n",
        "    \n",
        "    Returns:\n",
        "    cost - Tensor of the cost function\n",
        "    \"\"\"\n",
        "    \n",
        "    # to fit the tensorflow requirement for tf.nn.softmax_cross_entropy_with_logits(...,...)\n",
        "    logits = tf.transpose(Z3)\n",
        "    labels = tf.transpose(Y)\n",
        "    \n",
        "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = labels))\n",
        "    \n",
        "    return cost"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ha-wpsb_NpqM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def random_mini_batches(X, Y, mini_batch_size = 64, seed = 0):\n",
        "    \"\"\"\n",
        "    Creates a list of random minibatches from (X, Y)\n",
        "    \n",
        "    Arguments:\n",
        "    X -- input data, of shape (input size, number of examples)\n",
        "    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)\n",
        "    mini_batch_size - size of the mini-batches, integer\n",
        "    seed -- this is only for the purpose of grading, so that you're \"random minibatches are the same as ours.\n",
        "    \n",
        "    Returns:\n",
        "    mini_batches -- list of synchronous (mini_batch_X, mini_batch_Y)\n",
        "    \"\"\"\n",
        "    \n",
        "    m = X.shape[1]                  # number of training examples\n",
        "    mini_batches = []\n",
        "    np.random.seed(seed)\n",
        "    \n",
        "    # Step 1: Shuffle (X, Y)\n",
        "    permutation = list(np.random.permutation(m))\n",
        "    shuffled_X = X[:, permutation]\n",
        "    shuffled_Y = Y[:, permutation].reshape((Y.shape[0],m))\n",
        "\n",
        "    # Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.\n",
        "    num_complete_minibatches = math.floor(m/mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning\n",
        "    for k in range(0, num_complete_minibatches):\n",
        "        mini_batch_X = shuffled_X[:, k * mini_batch_size : k * mini_batch_size + mini_batch_size]\n",
        "        mini_batch_Y = shuffled_Y[:, k * mini_batch_size : k * mini_batch_size + mini_batch_size]\n",
        "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
        "        mini_batches.append(mini_batch)\n",
        "    \n",
        "    # Handling the end case (last mini-batch < mini_batch_size)\n",
        "    if m % mini_batch_size != 0:\n",
        "        mini_batch_X = shuffled_X[:, num_complete_minibatches * mini_batch_size : m]\n",
        "        mini_batch_Y = shuffled_Y[:, num_complete_minibatches * mini_batch_size : m]\n",
        "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
        "        mini_batches.append(mini_batch)\n",
        "    \n",
        "    return mini_batches"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "E8kXzSCaNG9M",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def model(X_train, Y_train, X_test, Y_test,dic_of_layer_sizes,learning_rate = 0.0001,\n",
        "          num_epochs = 1024, minibatch_size = 32, print_cost = True):\n",
        "    \"\"\"\n",
        "    Implements a three-layer tensorflow neural network: LINEAR->RELU->LINEAR->RELU->LINEAR->SOFTMAX.\n",
        "    \n",
        "    Arguments:\n",
        "    X_train -- training set, of shape (input size = 12288, number of training examples = 1080)\n",
        "    Y_train -- test set, of shape (output size = 6, number of training examples = 1080)\n",
        "    X_test -- training set, of shape (input size = 12288, number of training examples = 120)\n",
        "    Y_test -- test set, of shape (output size = 6, number of test examples = 120)\n",
        "    dic_of_layer_sizes - dictionary of W<n> b<n> sizes as [x,y]\n",
        "    learning_rate -- learning rate of the optimization\n",
        "    num_epochs -- number of epochs of the optimization loop\n",
        "    minibatch_size -- size of a minibatch\n",
        "    print_cost -- True to print the cost every 100 epochs\n",
        "    \n",
        "    Returns:\n",
        "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
        "    \"\"\"\n",
        "    \n",
        "    ops.reset_default_graph()                         # to be able to rerun the model without overwriting tf variables\n",
        "    tf.set_random_seed(1)                             # to keep consistent results\n",
        "    seed = 3                                          # to keep consistent results\n",
        "    (n_x, m) = X_train.shape                          # (n_x: input size, m : number of examples in the train set)\n",
        "    n_y = Y_train.shape[0]                            # n_y : output size\n",
        "    costs = []                                        # To keep track of the cost\n",
        "    \n",
        "    # Create Placeholders of shape (n_x, n_y)\n",
        "    X, Y = create_placeholders(n_x, n_y)\n",
        "\n",
        "    # Initialize parameters\n",
        "    parameters = initialize_parameters(dic_of_layer_sizes)\n",
        "    \n",
        "    # Forward propagation: Build the forward propagation in the tensorflow graph\n",
        "    Z3 = forward_propagation(X=X, parameters=parameters)\n",
        "    \n",
        "    # Cost function: Add cost function to tensorflow graph\n",
        "    cost = compute_cost(Y=Y,Z3=Z3)\n",
        "    \n",
        "    # Backpropagation: Define the tensorflow optimizer. Use an AdamOptimizer.\n",
        "    optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(cost)\n",
        "    \n",
        "    \n",
        "    # Initialize all the variables\n",
        "    init = tf.global_variables_initializer()\n",
        "\n",
        "    # Start the session to compute the tensorflow graph\n",
        "    with tf.Session() as sess:\n",
        "        \n",
        "        # Run the initialization\n",
        "        sess.run(init)\n",
        "        \n",
        "        # Do the training loop\n",
        "        for epoch in range(num_epochs):\n",
        "\n",
        "            epoch_cost = 0.                       # Defines a cost related to an epoch\n",
        "            num_minibatches = int(m / minibatch_size) # number of minibatches of size minibatch_size in the train set\n",
        "            seed = seed + 1\n",
        "            minibatches = random_mini_batches(X_train, Y_train, minibatch_size, seed)\n",
        "\n",
        "            for minibatch in minibatches:\n",
        "\n",
        "                # Select a minibatch\n",
        "                (minibatch_X, minibatch_Y) = minibatch\n",
        "                \n",
        "                # IMPORTANT: The line that runs the graph on a minibatch.\n",
        "                # Run the session to execute the \"optimizer\" and the \"cost\", the feedict should contain a minibatch for (X,Y).\n",
        "                \n",
        "                _ , minibatch_cost = sess.run([optimizer, cost], feed_dict={X: minibatch_X, Y: minibatch_Y})\n",
        "                \n",
        "                epoch_cost += minibatch_cost / num_minibatches\n",
        "\n",
        "            # Print the cost every epoch\n",
        "            if print_cost == True and epoch % 100 == 0:\n",
        "                print (\"Cost after epoch %i: %f\" % (epoch, epoch_cost))\n",
        "            if print_cost == True and epoch % 5 == 0:\n",
        "                costs.append(epoch_cost)\n",
        "                \n",
        "        # plot the cost\n",
        "        plt.plot(np.squeeze(costs))\n",
        "        plt.ylabel('cost')\n",
        "        plt.xlabel('iterations (per tens)')\n",
        "        plt.title(\"Learning rate =\" + str(learning_rate))\n",
        "        plt.show()\n",
        "\n",
        "        # lets save the parameters in a variable\n",
        "        parameters = sess.run(parameters)\n",
        "        print (\"Parameters have been trained!\")\n",
        "\n",
        "        # Calculate the correct predictions\n",
        "        correct_prediction = tf.equal(tf.argmax(Z3), tf.argmax(Y))\n",
        "\n",
        "        # Calculate accuracy on the test set\n",
        "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "\n",
        "        print (\"Train Accuracy:\", accuracy.eval({X: X_train, Y: Y_train}))\n",
        "        print (\"Test Accuracy:\", accuracy.eval({X: X_test, Y: Y_test}))\n",
        "        \n",
        "        return parameters"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DAiy879JNU7Y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1456
        },
        "outputId": "e8a49923-835d-4ce5-b804-ddadbb55e083"
      },
      "cell_type": "code",
      "source": [
        "training_size = 4*10000\n",
        "test_size = 4*2\n",
        "X_train, Y_train = generate_nand_data(training_size)\n",
        "Y_train = one_hot_matrix(Y_train.reshape(training_size), 2)\n",
        "X_test, Y_test = generate_nand_data(test_size)\n",
        "Y_test = one_hot_matrix(Y_test.reshape(test_size), 2)\n",
        "\n",
        "\n",
        "print (\"number of training examples = \" + str(X_train.shape[1]))\n",
        "print (\"number of test examples = \" + str(X_test.shape[1]))\n",
        "print (\"X_train shape: \" + str(X_train.shape))\n",
        "print (\"Y_train shape: \" + str(Y_train.shape))\n",
        "print (\"X_test shape: \" + str(X_test.shape))\n",
        "print (\"Y_test shape: \" + str(Y_test.shape))\n",
        "\n",
        "dic_of_layer_sizes ={\n",
        "    \"W1\":[8,2],\n",
        "    \"b1\":[8,1],\n",
        "    \"W2\":[2,8],\n",
        "    \"b2\":[2,1]\n",
        "}\n",
        "\n",
        "parameters = model(\n",
        "    X_train, \n",
        "    Y_train, \n",
        "    X_test, \n",
        "    Y_test, \n",
        "    dic_of_layer_sizes, \n",
        "    num_epochs=5000,\n",
        "    minibatch_size=1024\n",
        ")"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number of training examples = 40000\n",
            "number of test examples = 8\n",
            "X_train shape: (2, 40000)\n",
            "Y_train shape: (2, 40000)\n",
            "X_test shape: (2, 8)\n",
            "Y_test shape: (2, 8)\n",
            "W1\n",
            "[8, 2]\n",
            "b1\n",
            "[8, 1]\n",
            "W2\n",
            "[2, 8]\n",
            "b2\n",
            "[2, 1]\n",
            "Cost after epoch 0: 0.836004\n",
            "Cost after epoch 100: 0.350047\n",
            "Cost after epoch 200: 0.112368\n",
            "Cost after epoch 300: 0.024663\n",
            "Cost after epoch 400: 0.004635\n",
            "Cost after epoch 500: 0.000818\n",
            "Cost after epoch 600: 0.000139\n",
            "Cost after epoch 700: 0.000023\n",
            "Cost after epoch 800: 0.000004\n",
            "Cost after epoch 900: 0.000001\n",
            "Cost after epoch 1000: 0.000000\n",
            "Cost after epoch 1100: 0.000000\n",
            "Cost after epoch 1200: 0.000000\n",
            "Cost after epoch 1300: 0.000000\n",
            "Cost after epoch 1400: 0.000000\n",
            "Cost after epoch 1500: 0.000000\n",
            "Cost after epoch 1600: 0.000000\n",
            "Cost after epoch 1700: 0.000000\n",
            "Cost after epoch 1800: 0.000000\n",
            "Cost after epoch 1900: 0.000000\n",
            "Cost after epoch 2000: 0.000000\n",
            "Cost after epoch 2100: 0.000000\n",
            "Cost after epoch 2200: 0.000000\n",
            "Cost after epoch 2300: 0.000000\n",
            "Cost after epoch 2400: 0.000000\n",
            "Cost after epoch 2500: 0.000000\n",
            "Cost after epoch 2600: 0.000000\n",
            "Cost after epoch 2700: 0.000000\n",
            "Cost after epoch 2800: 0.000000\n",
            "Cost after epoch 2900: 0.000000\n",
            "Cost after epoch 3000: 0.000000\n",
            "Cost after epoch 3100: 0.000000\n",
            "Cost after epoch 3200: 0.000000\n",
            "Cost after epoch 3300: 0.000000\n",
            "Cost after epoch 3400: 0.000000\n",
            "Cost after epoch 3500: 0.000000\n",
            "Cost after epoch 3600: 0.000000\n",
            "Cost after epoch 3700: 0.000000\n",
            "Cost after epoch 3800: 0.000000\n",
            "Cost after epoch 3900: 0.000000\n",
            "Cost after epoch 4000: 0.000000\n",
            "Cost after epoch 4100: 0.000000\n",
            "Cost after epoch 4200: 0.000000\n",
            "Cost after epoch 4300: 0.000000\n",
            "Cost after epoch 4400: 0.000000\n",
            "Cost after epoch 4500: 0.000000\n",
            "Cost after epoch 4600: 0.000000\n",
            "Cost after epoch 4700: 0.000000\n",
            "Cost after epoch 4800: 0.000000\n",
            "Cost after epoch 4900: 0.000000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAEVCAYAAADpbDJPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f1a2c426a58>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Parameters have been trained!\n",
            "Train Accuracy: 1.0\n",
            "Test Accuracy: 1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "vPfDqZ9BoJ7L",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X, Y = generate_nand_data(2*4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bcFHp5ctoX4U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "034320ee-d090-4492-c9c2-3270f92593b7"
      },
      "cell_type": "code",
      "source": [
        "with tf.Session() as session:\n",
        " X = np.float32(X)\n",
        " z3 = forward_propagation(np.float32(X), parameters)\n",
        " p = tf.argmax(z3)\n",
        " print(X)\n",
        " print(session.run(p))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0. 0. 0. 0. 1. 1. 1. 1.]\n",
            " [0. 0. 1. 1. 0. 0. 1. 1.]]\n",
            "[1 1 1 1 1 1 0 0]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}