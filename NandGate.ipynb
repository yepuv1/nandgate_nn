{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NandGate.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/yepuv1/tensorflow/blob/master/NandGate.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "h35Fgz8M9Bie",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import math\n",
        "import numpy as np\n",
        "import h5py\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.python.framework import ops\n",
        "\n",
        "%matplotlib inline\n",
        "np.random.seed(1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jhBuGIkuJvXw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def nand(a, b):\n",
        "  \n",
        "  return np.logical_not(np.logical_and(a,b))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rXRLxSISMLN-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def generate_nand_data(examples_size, seed=1):\n",
        "  \n",
        "  assert(examples_size >= 4)\n",
        "  nand_input = np.array([[0,0],[0,1],[1,0],[1,1]], np.bool)\n",
        "  \n",
        "  features_size = 2\n",
        "  examples_size = examples_size - examples_size % 4\n",
        "  partition_size = np.int32(examples_size / 4)\n",
        "\n",
        "  X = np.zeros(shape=(features_size,examples_size), dtype=np.int32)\n",
        "  Y = np.zeros(shape=(1,examples_size), dtype=np.int32)\n",
        "  \n",
        "  for i in range(0,4):\n",
        "    s, e = (partition_size*i, partition_size*(i+1) )\n",
        "    X[:,s:e] = X[:,s:e] + nand_input[i].reshape(1,2).T\n",
        "  \n",
        "  Y = nand(X[0,:], X[1,:]).astype(np.int32).reshape(1,examples_size)\n",
        "  \n",
        "  return X, Y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-VpxQ1zMgtPt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def one_hot_matrix(labels, C):\n",
        "    \"\"\"\n",
        "    Creates a matrix where the i-th row corresponds to the ith class number and the jth column\n",
        "                     corresponds to the jth training example. So if example j had a label i. Then entry (i,j) \n",
        "                     will be 1. \n",
        "                     \n",
        "    Arguments:\n",
        "    labels -- vector containing the labels \n",
        "    C -- number of classes, the depth of the one hot dimension\n",
        "    \n",
        "    Returns: \n",
        "    one_hot -- one hot matrix\n",
        "    \"\"\"\n",
        "    \n",
        "    C = tf.constant(dtype=tf.int32, name=\"C\", value=C)\n",
        "    one_hot_matrix = tf.one_hot(labels,depth=C, axis=0)\n",
        "    with tf.Session() as sess:\n",
        "      one_hot = sess.run(one_hot_matrix)\n",
        "   \n",
        "    return one_hot"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ynl7o7519uSR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def create_placeholders(n_x, n_y):\n",
        "    \"\"\"\n",
        "    Creates the placeholders for the tensorflow session.\n",
        "    \n",
        "    Arguments:\n",
        "    n_x -- scalar, size of input vector \n",
        "    n_y -- scalar, number of classes (from 0 to 1, so -> 2)\n",
        "    \n",
        "    Returns:\n",
        "    X -- placeholder for the data input, of shape [n_x, None] and dtype \"float\"\n",
        "    Y -- placeholder for the input labels, of shape [n_y, None] and dtype \"float\"\n",
        "    \n",
        "    Tips:\n",
        "    - You will use None because it let's us be flexible on the number of examples you will for the placeholders.\n",
        "      In fact, the number of examples during test/train is different.\n",
        "    \"\"\"\n",
        "\n",
        "    X = tf.placeholder(tf.float32, shape=(n_x, None))\n",
        "    Y = tf.placeholder(tf.float32, shape=(n_y, None))\n",
        "\n",
        "    \n",
        "    return X, Y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "auX2nNLs_F7D",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def initialize_parameters(dic_of_layer_sizes):\n",
        "    \"\"\"\n",
        "    Initializes parameters to build a neural network with tensorflow. The shapes are:\n",
        "                        W1 : [25, 2]\n",
        "                        b1 : [25, 1]\n",
        "                        W2 : [12, 25]\n",
        "                        b2 : [12, 1]\n",
        "                        W3 : [6, 12]\n",
        "                        b3 : [6, 1]\n",
        "    \n",
        "    Returns:\n",
        "    parameters -- a dictionary of tensors containing W1, b1, W2, b2, W3, b3\n",
        "    \"\"\"\n",
        "    \n",
        "    tf.set_random_seed(1) \n",
        "        \n",
        "    \n",
        "    parameters ={}\n",
        "    for key, value in dic_of_layer_sizes.items():\n",
        "      print(key)\n",
        "      print(value)\n",
        "      parameters[key]=tf.get_variable(key, value, initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
        "    \n",
        "    \n",
        "    return parameters"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6ucbcR9OK_ur",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def forward_propagation(X, parameters):\n",
        "    \"\"\"\n",
        "    Implements the forward propagation for the model: LINEAR -> RELU -> LINEAR ->  SOFTMAX\n",
        "    \n",
        "    Arguments:\n",
        "    X -- input dataset placeholder, of shape (input size, number of examples)\n",
        "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", \"W2\", \"b2\", \"W3\", \"b3\"\n",
        "                  the shapes are given in initialize_parameters\n",
        "\n",
        "    Returns:\n",
        "    Z3 -- the output of the last LINEAR unit\n",
        "    \"\"\"\n",
        "    \n",
        "    # Retrieve the parameters from the dictionary \"parameters\" \n",
        "    W1 = parameters['W1']\n",
        "    b1 = parameters['b1']\n",
        "    W2 = parameters['W2']\n",
        "    b2 = parameters['b2']\n",
        "              \n",
        "    Z1 = tf.add(tf.matmul(W1, X), b1)  # LINEAR                                         \n",
        "    A1 = tf.nn.relu(Z1)                # RELU                           \n",
        "    Z2 = tf.add(tf.matmul(W2, A1), b2) # LINEAR                                           \n",
        "\n",
        "    return Z2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "O2HSASNFMu45",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def compute_cost(Z3, Y):\n",
        "    \"\"\"\n",
        "    Computes the cost\n",
        "    \n",
        "    Arguments:\n",
        "    Z3 -- output of forward propagation (output of the last LINEAR unit), of shape (6, number of examples)\n",
        "    Y -- \"true\" labels vector placeholder, same shape as Z3\n",
        "    \n",
        "    Returns:\n",
        "    cost - Tensor of the cost function\n",
        "    \"\"\"\n",
        "    \n",
        "    # to fit the tensorflow requirement for tf.nn.softmax_cross_entropy_with_logits(...,...)\n",
        "    logits = tf.transpose(Z3)\n",
        "    labels = tf.transpose(Y)\n",
        "    \n",
        "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = labels))\n",
        "    \n",
        "    return cost"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ha-wpsb_NpqM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def random_mini_batches(X, Y, mini_batch_size = 64, seed = 0):\n",
        "    \"\"\"\n",
        "    Creates a list of random minibatches from (X, Y)\n",
        "    \n",
        "    Arguments:\n",
        "    X -- input data, of shape (input size, number of examples)\n",
        "    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)\n",
        "    mini_batch_size - size of the mini-batches, integer\n",
        "    seed -- this is only for the purpose of grading, so that you're \"random minibatches are the same as ours.\n",
        "    \n",
        "    Returns:\n",
        "    mini_batches -- list of synchronous (mini_batch_X, mini_batch_Y)\n",
        "    \"\"\"\n",
        "    \n",
        "    m = X.shape[1]                  # number of training examples\n",
        "    mini_batches = []\n",
        "    np.random.seed(seed)\n",
        "    \n",
        "    # Step 1: Shuffle (X, Y)\n",
        "    permutation = list(np.random.permutation(m))\n",
        "    shuffled_X = X[:, permutation]\n",
        "    shuffled_Y = Y[:, permutation].reshape((Y.shape[0],m))\n",
        "\n",
        "    # Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.\n",
        "    num_complete_minibatches = math.floor(m/mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning\n",
        "    for k in range(0, num_complete_minibatches):\n",
        "        mini_batch_X = shuffled_X[:, k * mini_batch_size : k * mini_batch_size + mini_batch_size]\n",
        "        mini_batch_Y = shuffled_Y[:, k * mini_batch_size : k * mini_batch_size + mini_batch_size]\n",
        "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
        "        mini_batches.append(mini_batch)\n",
        "    \n",
        "    # Handling the end case (last mini-batch < mini_batch_size)\n",
        "    if m % mini_batch_size != 0:\n",
        "        mini_batch_X = shuffled_X[:, num_complete_minibatches * mini_batch_size : m]\n",
        "        mini_batch_Y = shuffled_Y[:, num_complete_minibatches * mini_batch_size : m]\n",
        "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
        "        mini_batches.append(mini_batch)\n",
        "    \n",
        "    return mini_batches"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "E8kXzSCaNG9M",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def model(X_train, Y_train, X_test, Y_test,dic_of_layer_sizes,learning_rate = 0.0001,\n",
        "          num_epochs = 1024, minibatch_size = 32, print_cost = True):\n",
        "    \"\"\"\n",
        "    Implements a three-layer tensorflow neural network: LINEAR->RELU->LINEAR->RELU->LINEAR->SOFTMAX.\n",
        "    \n",
        "    Arguments:\n",
        "    X_train -- training set, of shape (input size = 12288, number of training examples = 1080)\n",
        "    Y_train -- test set, of shape (output size = 6, number of training examples = 1080)\n",
        "    X_test -- training set, of shape (input size = 12288, number of training examples = 120)\n",
        "    Y_test -- test set, of shape (output size = 6, number of test examples = 120)\n",
        "    dic_of_layer_sizes - dictionary of W<n> b<n> sizes as [x,y]\n",
        "    learning_rate -- learning rate of the optimization\n",
        "    num_epochs -- number of epochs of the optimization loop\n",
        "    minibatch_size -- size of a minibatch\n",
        "    print_cost -- True to print the cost every 100 epochs\n",
        "    \n",
        "    Returns:\n",
        "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
        "    \"\"\"\n",
        "    \n",
        "    ops.reset_default_graph()                         # to be able to rerun the model without overwriting tf variables\n",
        "    tf.set_random_seed(1)                             # to keep consistent results\n",
        "    seed = 3                                          # to keep consistent results\n",
        "    (n_x, m) = X_train.shape                          # (n_x: input size, m : number of examples in the train set)\n",
        "    n_y = Y_train.shape[0]                            # n_y : output size\n",
        "    costs = []                                        # To keep track of the cost\n",
        "    \n",
        "    # Create Placeholders of shape (n_x, n_y)\n",
        "    X, Y = create_placeholders(n_x, n_y)\n",
        "\n",
        "    # Initialize parameters\n",
        "    parameters = initialize_parameters(dic_of_layer_sizes)\n",
        "    \n",
        "    # Forward propagation: Build the forward propagation in the tensorflow graph\n",
        "    Z3 = forward_propagation(X=X, parameters=parameters)\n",
        "    \n",
        "    # Cost function: Add cost function to tensorflow graph\n",
        "    cost = compute_cost(Y=Y,Z3=Z3)\n",
        "    \n",
        "    # Backpropagation: Define the tensorflow optimizer. Use an AdamOptimizer.\n",
        "    optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(cost)\n",
        "    \n",
        "    \n",
        "    # Initialize all the variables\n",
        "    init = tf.global_variables_initializer()\n",
        "\n",
        "    # Start the session to compute the tensorflow graph\n",
        "    with tf.Session() as sess:\n",
        "        \n",
        "        # Run the initialization\n",
        "        sess.run(init)\n",
        "        \n",
        "        # Do the training loop\n",
        "        for epoch in range(num_epochs):\n",
        "\n",
        "            epoch_cost = 0.                       # Defines a cost related to an epoch\n",
        "            num_minibatches = int(m / minibatch_size) # number of minibatches of size minibatch_size in the train set\n",
        "            seed = seed + 1\n",
        "            minibatches = random_mini_batches(X_train, Y_train, minibatch_size, seed)\n",
        "\n",
        "            for minibatch in minibatches:\n",
        "\n",
        "                # Select a minibatch\n",
        "                (minibatch_X, minibatch_Y) = minibatch\n",
        "                \n",
        "                # IMPORTANT: The line that runs the graph on a minibatch.\n",
        "                # Run the session to execute the \"optimizer\" and the \"cost\", the feedict should contain a minibatch for (X,Y).\n",
        "                \n",
        "                _ , minibatch_cost = sess.run([optimizer, cost], feed_dict={X: minibatch_X, Y: minibatch_Y})\n",
        "                \n",
        "                epoch_cost += minibatch_cost / num_minibatches\n",
        "\n",
        "            # Print the cost every epoch\n",
        "            if print_cost == True and epoch % 100 == 0:\n",
        "                print (\"Cost after epoch %i: %f\" % (epoch, epoch_cost))\n",
        "            if print_cost == True and epoch % 5 == 0:\n",
        "                costs.append(epoch_cost)\n",
        "                \n",
        "        # plot the cost\n",
        "        plt.plot(np.squeeze(costs))\n",
        "        plt.ylabel('cost')\n",
        "        plt.xlabel('iterations (per tens)')\n",
        "        plt.title(\"Learning rate =\" + str(learning_rate))\n",
        "        plt.show()\n",
        "\n",
        "        # lets save the parameters in a variable\n",
        "        parameters = sess.run(parameters)\n",
        "        print (\"Parameters have been trained!\")\n",
        "\n",
        "        # Calculate the correct predictions\n",
        "        correct_prediction = tf.equal(tf.argmax(Z3), tf.argmax(Y))\n",
        "\n",
        "        # Calculate accuracy on the test set\n",
        "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "\n",
        "        print (\"Train Accuracy:\", accuracy.eval({X: X_train, Y: Y_train}))\n",
        "        print (\"Test Accuracy:\", accuracy.eval({X: X_test, Y: Y_test}))\n",
        "        \n",
        "        return parameters"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DAiy879JNU7Y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 753
        },
        "outputId": "f0f3b084-5355-4942-8f11-e0a3fe3c423e"
      },
      "cell_type": "code",
      "source": [
        "training_size = 4*1000\n",
        "test_size = 4*2\n",
        "X_train, Y_train = generate_nand_data(training_size)\n",
        "Y_train = one_hot_matrix(Y_train.reshape(training_size), 2)\n",
        "X_test, Y_test = generate_nand_data(test_size)\n",
        "Y_test = one_hot_matrix(Y_test.reshape(test_size), 2)\n",
        "\n",
        "\n",
        "print (\"number of training examples = \" + str(X_train.shape[1]))\n",
        "print (\"number of test examples = \" + str(X_test.shape[1]))\n",
        "print (\"X_train shape: \" + str(X_train.shape))\n",
        "print (\"Y_train shape: \" + str(Y_train.shape))\n",
        "print (\"X_test shape: \" + str(X_test.shape))\n",
        "print (\"Y_test shape: \" + str(Y_test.shape))\n",
        "\n",
        "dic_of_layer_sizes ={\n",
        "    \"W1\":[8,2],\n",
        "    \"b1\":[8,1],\n",
        "    \"W2\":[2,8],\n",
        "    \"b2\":[2,1]\n",
        "}\n",
        "\n",
        "parameters = model(\n",
        "    X_train, \n",
        "    Y_train, \n",
        "    X_test, \n",
        "    Y_test, \n",
        "    dic_of_layer_sizes, \n",
        "    num_epochs=1000,\n",
        "    minibatch_size=64\n",
        ")"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number of training examples = 4000\n",
            "number of test examples = 8\n",
            "X_train shape: (2, 4000)\n",
            "Y_train shape: (2, 4000)\n",
            "X_test shape: (2, 8)\n",
            "Y_test shape: (2, 8)\n",
            "W1\n",
            "[8, 2]\n",
            "b1\n",
            "[8, 1]\n",
            "W2\n",
            "[2, 8]\n",
            "b2\n",
            "[2, 1]\n",
            "Cost after epoch 0: 0.825064\n",
            "Cost after epoch 100: 0.211933\n",
            "Cost after epoch 200: 0.031326\n",
            "Cost after epoch 300: 0.003014\n",
            "Cost after epoch 400: 0.000241\n",
            "Cost after epoch 500: 0.000017\n",
            "Cost after epoch 600: 0.000001\n",
            "Cost after epoch 700: 0.000000\n",
            "Cost after epoch 800: 0.000000\n",
            "Cost after epoch 900: 0.000000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAEVCAYAAADpbDJPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nOXYqxQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f19fb102a58>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Parameters have been trained!\n",
            "Train Accuracy: 1.0\n",
            "Test Accuracy: 1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "vPfDqZ9BoJ7L",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X, Y = generate_nand_data(2*4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bcFHp5ctoX4U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "1be8cca6-568c-4d3e-cc96-a8db2f0c0475"
      },
      "cell_type": "code",
      "source": [
        "with tf.Session() as session:\n",
        " X = np.float32(X)\n",
        " z3 = forward_propagation(np.float32(X), parameters)\n",
        " p = tf.argmax(z3)\n",
        " print(X)\n",
        " print(session.run(p))"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0. 0. 0. 0. 1. 1. 1. 1.]\n",
            " [0. 0. 1. 1. 0. 0. 1. 1.]]\n",
            "[1 1 1 1 1 1 0 0]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}